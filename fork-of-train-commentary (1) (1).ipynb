{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T14:06:12.533299Z",
     "iopub.status.busy": "2025-07-18T14:06:12.532964Z",
     "iopub.status.idle": "2025-07-18T14:06:23.539143Z",
     "shell.execute_reply": "2025-07-18T14:06:23.538541Z",
     "shell.execute_reply.started": "2025-07-18T14:06:12.533268Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  Import all necessary libraries\n",
    "import torch\n",
    "import clip\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW # Corrected import\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T14:07:32.906951Z",
     "iopub.status.busy": "2025-07-18T14:07:32.905936Z",
     "iopub.status.idle": "2025-07-18T14:07:32.935022Z",
     "shell.execute_reply": "2025-07-18T14:07:32.934286Z",
     "shell.execute_reply.started": "2025-07-18T14:07:32.906926Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CricketCommentaryDataset(Dataset):\n",
    "    def __init__(self, annotations, clip_model, preprocess, num_frames=16):\n",
    "        self.annotations = annotations\n",
    "        self.clip_model = clip_model\n",
    "        self.preprocess = preprocess\n",
    "        self.num_frames = num_frames\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def extract_frames(self, video_path, start_time, end_time):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(\"video not opened\")\n",
    "            return torch.zeros(self.num_frames, 3, 224, 224)\n",
    "\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        start_frame = int(start_time * fps)\n",
    "        end_frame = int(end_time * fps)\n",
    "\n",
    "        if start_frame >= end_frame:\n",
    "            return torch.zeros(self.num_frames, 3, 224, 224)\n",
    "\n",
    "        stride = max(1, (end_frame - start_frame) // self.num_frames)\n",
    "        frames = []\n",
    "\n",
    "        for i in range(start_frame, end_frame, stride):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                # Action-focused cropping\n",
    "                h, w, _ = frame.shape\n",
    "                crop_size = min(h, w) // 2\n",
    "                y_start = max(0, (h - crop_size) // 2)\n",
    "                x_start = max(0, (w - crop_size) // 2)\n",
    "                cropped = frame[y_start:y_start+crop_size, x_start:x_start+crop_size]\n",
    "\n",
    "                cropped = cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB)\n",
    "                pil_image = Image.fromarray(cropped)\n",
    "                frames.append(self.preprocess(pil_image))\n",
    "            if len(frames) >= self.num_frames:\n",
    "                break\n",
    "        \n",
    "        # Always ensure we return exactly num_frames\n",
    "        if len(frames) < self.num_frames:\n",
    "            num_pad = self.num_frames - len(frames)\n",
    "            frames.extend([torch.zeros(3, 224, 224)] * num_pad)\n",
    "\n",
    "        cap.release()\n",
    "        return torch.stack(frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ann = self.annotations[idx]\n",
    "        frames = self.extract_frames(\n",
    "            ann[\"video_path\"],\n",
    "            ann[\"start_time\"],\n",
    "            ann[\"end_time\"]\n",
    "        )\n",
    "\n",
    "        # Use the prompt and response directly\n",
    "        prompt = ann[\"prompt\"]\n",
    "        response = ann[\"response\"]\n",
    "\n",
    "        return {\n",
    "            \"frames\": frames,\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response\n",
    "        }\n",
    "    \n",
    "class TemporalTransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, num_frames, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "        self.position_embed = nn.Parameter(torch.zeros(1, num_frames + 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.position_embed, std=0.02)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=4 * embed_dim,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        cls_token = self.cls_token.expand(B, 1, -1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.position_embed[:, :x.size(1)]\n",
    "        x = self.transformer(x)\n",
    "        return {\n",
    "            \"cls\": x[:, 0],\n",
    "            \"tokens\": x[:, 1:]\n",
    "        }\n",
    "class CricketCommentator(nn.Module):\n",
    "    def __init__(self, train_mode=False, num_frames=16, train_layers=2):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        import clip\n",
    "        self.clip, self.preprocess = clip.load(\"ViT-B/32\", device=self.device)\n",
    "        self.clip = self.clip.float()\n",
    "\n",
    "        if train_mode:\n",
    "            for param in self.clip.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.temporal_encoder = TemporalTransformerEncoder(\n",
    "            embed_dim=512,\n",
    "            num_heads=8,\n",
    "            num_layers=3,\n",
    "            num_frames=num_frames,\n",
    "            dropout=0.1\n",
    "        ).to(self.device).float()\n",
    "\n",
    "        # Updated projection for DeepSeek (2048-dim)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(512, 2048),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.Tanh()\n",
    "        ).to(self.device).float()\n",
    "\n",
    "        # DeepSeek model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\").to(self.device).float()\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Freeze all parameters initially\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze last N layers if training\n",
    "        if train_mode and train_layers > 0:\n",
    "            # Unfreeze last transformer blocks\n",
    "            for block in self.model.model.layers[-train_layers:]:\n",
    "                for param in block.parameters():\n",
    "                    param.requires_grad = True\n",
    "            \n",
    "            # Unfreeze final norm and head\n",
    "            for param in self.model.model.norm.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in self.model.lm_head.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, frames):\n",
    "        batch_size = frames.shape[0]\n",
    "        frames = frames.view(-1, 3, 224, 224)\n",
    "        with torch.no_grad():\n",
    "            frame_features = self.clip.encode_image(frames.to(self.device))\n",
    "        frame_features = frame_features.view(batch_size, self.num_frames, -1).float()\n",
    "        frame_features = F.normalize(frame_features, p=2, dim=-1)\n",
    "\n",
    "        temporal_out = self.temporal_encoder(frame_features)\n",
    "        visual_embeds = self.projection(temporal_out[\"cls\"])\n",
    "        return F.normalize(visual_embeds, p=2, dim=-1).unsqueeze(1)\n",
    "\n",
    "    def compute_loss(self, batch):\n",
    "        frames = batch[\"frames\"].to(self.device)\n",
    "        prompts = batch[\"prompt\"]\n",
    "        responses = batch[\"response\"]\n",
    "\n",
    "        visual_embeds = self.forward(frames)  # [batch_size, 1, 2048]\n",
    "\n",
    "        full_texts = [f\"{p} {r}\" for p, r in zip(prompts, responses)]\n",
    "        inputs = self.tokenizer(\n",
    "            full_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding='longest',\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(self.device)\n",
    "\n",
    "        prompt_inputs = self.tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding='longest',\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(self.device)\n",
    "        prompt_lengths = prompt_inputs.attention_mask.sum(dim=1)\n",
    "\n",
    "        # Get text embeddings using DeepSeek's embedding layer\n",
    "        text_embeddings = self.model.model.embed_tokens(inputs.input_ids)\n",
    "        \n",
    "        # Concatenate visual and text embeddings\n",
    "        input_embeddings = torch.cat([visual_embeds, text_embeddings], dim=1)\n",
    "        \n",
    "        # Create attention mask for visual part\n",
    "        visual_mask = torch.ones(visual_embeds.shape[:2]).to(self.device)\n",
    "        combined_mask = torch.cat([visual_mask, inputs.attention_mask], dim=1)\n",
    "        \n",
    "        # Create labels (-100 for visual token and prompt)\n",
    "        labels = inputs.input_ids.clone()\n",
    "        extended_labels = torch.cat([\n",
    "            -100 * torch.ones(labels.size(0), 1, dtype=torch.long).to(self.device),\n",
    "            labels\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Mask prompt text in labels\n",
    "        for i, plen in enumerate(prompt_lengths):\n",
    "            extended_labels[i, 1:1+plen] = -100\n",
    "\n",
    "        outputs = self.model(\n",
    "            inputs_embeds=input_embeddings,\n",
    "            attention_mask=combined_mask,\n",
    "            labels=extended_labels\n",
    "        )\n",
    "        return outputs.loss\n",
    "def collate_fn(batch):\n",
    "    \n",
    "    \"\"\"Custom collate function to handle frames\"\"\"\n",
    "    frames = [item[\"frames\"] for item in batch]\n",
    "    prompts = [item[\"prompt\"] for item in batch]\n",
    "    responses = [item[\"response\"] for item in batch]\n",
    "    \n",
    "    # Stack all frames\n",
    "    frames_tensor = torch.stack(frames)\n",
    "    \n",
    "    return {\n",
    "        \"frames\": frames_tensor,\n",
    "        \"prompt\": prompts,\n",
    "        \"response\": responses\n",
    "    }\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, lr):\n",
    "    device = model.device\n",
    "    scaler = torch.cuda.amp.GradScaler()  # Mixed precision\n",
    "\n",
    "    # Group parameters for different learning rates\n",
    "    temporal_params = list(model.temporal_encoder.parameters())\n",
    "    proj_params = list(model.projection.parameters())\n",
    "    deepseek_trainable = [p for p in model.model.parameters() if p.requires_grad]\n",
    "\n",
    "    optimizer = AdamW([\n",
    "        {'params': temporal_params, 'lr': lr},\n",
    "        {'params': proj_params, 'lr': lr},\n",
    "        {'params': deepseek_trainable, 'lr': lr * 0.1}\n",
    "    ], weight_decay=0.01)\n",
    "\n",
    "    plateau_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "    accum_steps = 4  # Gradient accumulation\n",
    "    total_steps = len(train_loader) * epochs // accum_steps\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = 5  # For early stopping\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        total_train_loss = 0.0\n",
    "        step_count = 0\n",
    "\n",
    "        for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss = model.compute_loss(batch)\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            step_count += 1\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (i + 1) % accum_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / step_count\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                loss = model.compute_loss(batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        plateau_scheduler.step(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement for {epochs_no_improve} epoch(s).\")\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "                break\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T14:07:38.926405Z",
     "iopub.status.busy": "2025-07-18T14:07:38.925859Z",
     "iopub.status.idle": "2025-07-18T14:08:23.655785Z",
     "shell.execute_reply": "2025-07-18T14:08:23.655170Z",
     "shell.execute_reply.started": "2025-07-18T14:07:38.926372Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 🖥️ Set the computation device (GPU if available)\n",
    "# Load annotations\n",
    "with open(\"final_data/Data_updated_1.json\", \"r\") as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Split into train and validation (85/15)\n",
    "split_idx = int(0.85 * len(annotations))\n",
    "train_annotations = annotations[:split_idx]\n",
    "val_annotations = annotations[split_idx:]\n",
    "\n",
    "# Initialize CLIP for dataset\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CricketCommentaryDataset(\n",
    "    train_annotations,\n",
    "    clip_model,\n",
    "    preprocess,\n",
    "    num_frames=16\n",
    ")\n",
    "val_dataset = CricketCommentaryDataset(\n",
    "    val_annotations,\n",
    "    clip_model,\n",
    "    preprocess,\n",
    "    num_frames=16\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,  # Small batch size due to memory constraints\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# Initialize model in training mode\n",
    "model = CricketCommentator(train_mode=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T14:10:28.376668Z",
     "iopub.status.busy": "2025-07-18T14:10:28.375548Z",
     "iopub.status.idle": "2025-07-18T17:20:45.293874Z",
     "shell.execute_reply": "2025-07-18T17:20:45.292702Z",
     "shell.execute_reply.started": "2025-07-18T14:10:28.376641Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75947/1057914550.py:239: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()  # Mixed precision\n",
      "Epoch 1:   0%|          | 0/347 [00:00<?, ?it/s]/tmp/ipykernel_75947/1057914550.py:274: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1:   1%|          | 3/347 [00:12<18:46,  3.28s/it]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████| 347/347 [17:19<00:00,  3.00s/it]\n",
      "Validation:   0%|          | 0/62 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|██████████| 62/62 [03:49<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 5.1215 | Val Loss: 4.5680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/347 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 2: 100%|██████████| 347/347 [16:59<00:00,  2.94s/it]\n",
      "Validation:   0%|          | 0/62 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|██████████| 62/62 [03:48<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 4.0203 | Val Loss: 3.8988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/347 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 3: 100%|██████████| 347/347 [17:03<00:00,  2.95s/it]\n",
      "Validation:   0%|          | 0/62 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|██████████| 62/62 [03:44<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 3.3674 | Val Loss: 3.4583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/347 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 4: 100%|██████████| 347/347 [16:59<00:00,  2.94s/it]\n",
      "Validation:   0%|          | 0/62 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|██████████| 62/62 [03:44<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 2.9322 | Val Loss: 3.2787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:   0%|          | 0/347 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 5: 100%|██████████| 347/347 [17:04<00:00,  2.95s/it]\n",
      "Validation:   0%|          | 0/62 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|██████████| 62/62 [03:45<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 2.5950 | Val Loss: 3.1927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   0%|          | 0/347 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 6: 100%|██████████| 347/347 [17:03<00:00,  2.95s/it]\n",
      "Validation:   0%|          | 0/62 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation:  98%|█████████▊| 61/62 [03:45<00:03,  3.73s/it]TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|██████████| 62/62 [03:45<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 2.3214 | Val Loss: 3.1988\n",
      "No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:   0%|          | 0/347 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 7: 100%|██████████| 347/347 [17:00<00:00,  2.94s/it]\n",
      "Validation:   0%|          | 0/62 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|██████████| 62/62 [03:46<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 2.0786 | Val Loss: 3.1862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8:   0%|          | 0/347 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 8: 100%|██████████| 347/347 [16:54<00:00,  2.92s/it]\n",
      "Validation:   0%|          | 0/62 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|██████████| 62/62 [03:44<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 1.8447 | Val Loss: 3.2497\n",
      "No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:   0%|          | 0/347 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 9: 100%|██████████| 347/347 [16:56<00:00,  2.93s/it]\n",
      "Validation:   0%|          | 0/62 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|██████████| 62/62 [03:45<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 1.6383 | Val Loss: 3.3047\n",
      "No improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10:   0%|          | 0/347 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 10: 100%|██████████| 347/347 [16:58<00:00,  2.94s/it]\n",
      "Validation:   0%|          | 0/62 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|██████████| 62/62 [03:44<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 1.4548 | Val Loss: 3.3247\n",
      "No improvement for 3 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11:   0%|          | 0/347 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 11: 100%|██████████| 347/347 [16:56<00:00,  2.93s/it]\n",
      "Validation:   0%|          | 0/62 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|██████████| 62/62 [03:44<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 1.2955 | Val Loss: 3.4490\n",
      "No improvement for 4 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12:   0%|          | 0/347 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 12: 100%|██████████| 347/347 [17:00<00:00,  2.94s/it]\n",
      "Validation:   0%|          | 0/62 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Validation: 100%|██████████| 62/62 [03:44<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 1.1449 | Val Loss: 3.4889\n",
      "No improvement for 5 epoch(s).\n",
      "Early stopping triggered after 12 epochs.\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# 🚂 Training loop to fine-tune the model\n",
    "# Train the model\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "trained_model = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=30,\n",
    "    lr=1e-4,\n",
    "\n",
    ")\n",
    "  # Save final model\n",
    "torch.save(trained_model.state_dict(), \"cricket_commentator_final.pth\")\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7711526,
     "sourceId": 12320630,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
